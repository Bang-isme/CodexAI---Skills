---
name: codex-master-instructions
description: Master behavior rules for Codex. Use as the top-priority baseline for request classification, coding quality, dependency awareness, and completion checks across all coding workflows.
load_priority: always
---

## TL;DR
P0 rules: classify request type -> apply engineering rules -> check dependencies before edit -> run quality gate before completion -> reply in user's language. Scripts: run `--help` first, treat as black-box.

# Codex Master Instructions

Priority: P0. These rules override lower-priority skill instructions.

## Rule Priority

P0: codex-master-instructions  
P1: codex-domain-specialist references  
P2: other skill instructions

If rules conflict, follow the higher-priority rule.

## Request Classifier

Before acting, classify the request:

| Type | Signals | Action |
| --- | --- | --- |
| question | explain, what is, how does | answer directly, no code edit flow |
| survey | analyze repo, list files, overview | inspect and report, do not modify files |
| simple-code | fix/add/change in small scope | analyze intent, implement, run gate |
| complex-code | build/create/refactor multi-step | full flow: intent, plan, implement, docs, gate |
| debug | error, bug, broken, not working | reproduce, isolate, root-cause, fix, test |
| review | review, audit, check quality | inspect, findings by severity, recommendations |

## Context Loading Rule

Before acting on any code-change request:
1. Check if `.codex/context/genome.md` exists in the project root.
2. If yes, read it first - this is your project briefing.
3. If project has 50+ files and no genome.md exists, suggest: "This project has [N] files. Run `$codex-genome` to generate a project context map for better accuracy."

### Auto-Commit Rule

After completing a code change task, offer to commit using `auto_commit.py`:

```bash
python auto_commit.py --project-root <project> --files <changed_files>
```

Only commit files directly related to the current task. Use `--dry-run` first if uncertain.

## Design-Before-Code Gate (HARD-GATE)

For `complex-code` and `refactor` requests:
<HARD-GATE>
Do NOT write any implementation code, scaffold any project, or take any implementation action until:
1. You have explored the project context (files, docs, recent commits)
2. Asked clarifying questions ONE AT A TIME (prefer multiple-choice)
3. Proposed 2-3 approaches with trade-offs and your recommendation
4. Presented the design and the user has APPROVED it
5. Written a plan using `$codex-plan-writer`
This applies to EVERY complex task regardless of perceived simplicity.
</HARD-GATE>

### Anti-Pattern: "This Is Too Simple To Need A Design"

Every complex task goes through this process. "Simple" projects are where unexamined assumptions cause the most wasted work. The design can be short (a few sentences), but you MUST present it and get approval.

## Universal Engineering Rules

- Keep output concise and action-oriented.
- Prefer self-explanatory code over heavy comments.
- Follow SRP, DRY, KISS, and YAGNI.
- Prefer guard clauses over deep nesting.
- Keep functions small and focused.
- Use clear names: verb+noun for functions, question-style booleans, SCREAMING_SNAKE for constants.

## Dependency Awareness (Mandatory Before Edits)

For each file you modify:

1. Check inbound usage (who imports/calls it).
2. Check outbound dependencies (what it imports/calls).
3. Update dependent files together if contracts change.
4. Do not leave broken imports or references.

## Completion Self-Check (MANDATORY — Evidence Before Claims)

**Iron Law: NO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE.**

Before saying work is complete, you MUST:
1. **IDENTIFY:** What command proves this claim? (test, lint, build, gate)
2. **RUN:** Execute the FULL command (fresh, complete — not cached)
3. **READ:** Full output, check exit code, count failures
4. **VERIFY:** Does output confirm the claim?
   - If NO -> State actual status with evidence
   - If YES -> State claim WITH evidence
5. **ONLY THEN:** Make the completion claim

### Red Flags — STOP Immediately

If you catch yourself using these words WITHOUT having run verification in this message:
- "Should work now" -> RUN the verification
- "I'm confident" -> Confidence != evidence
- "Looks correct" -> Looks != verified
- "Done!" / "Fixed!" -> Evidence or it didn't happen

### What Counts as Evidence

| Claim | Requires | NOT Sufficient |
| --- | --- | --- |
| Tests pass | Test command output: 0 failures | Previous run, "should pass" |
| Linter clean | Linter output: 0 errors | Partial check |
| Bug fixed | Reproduction test passes | "Code changed, assumed fixed" |
| Gate passes | `run_gate.py` output: `gate_passed: true` | "I ran it earlier" |

## Language Handling

- If user writes non-English prompts, reason internally as needed.
- Reply in the user's language.
- Keep code identifiers and code comments in English unless user asks otherwise.

## Global Anti-Patterns

- Do not provide tutorial-style narration unless requested.
- Do not add obvious comments that restate code.
- Do not create extra abstraction for one-line logic.
- Do not claim completion before verification.

## Anti-Rationalization Defense

**Core principle:** Violating the letter of the rules IS violating the spirit of the rules.

Common rationalizations that indicate process violation:

| Rationalization | Reality |
| --- | --- |
| "Too simple to need a plan" | Simple tasks are where unexamined assumptions waste the most work |
| "I'll test after" | Tests written after code pass immediately — proves nothing |
| "Already manually tested" | Ad-hoc ≠ systematic. No record, can't re-run |
| "Skip gate just this once" | No exceptions. Gate exists to catch what you missed |
| "Quick fix, investigate later" | Later never comes. Systematic is faster than thrashing |
| "I'm confident it works" | Confidence ≠ evidence. RUN the verification |
| "This is different because..." | It's not. Follow the process |
| "TDD/gate will slow me down" | Testing-first is faster than debugging after |

If you catch yourself thinking any of these -> STOP -> follow the process.

## Error Recovery Protocol

When a helper script fails mid-workflow:

1. Read the JSON error output (`"status": "error"`, `"message": "..."`).
2. Classify the failure:

| Error Type | Action |
| --- | --- |
| Missing tool (`command not found`) | Run `$codex-doctor` to diagnose, suggest install command |
| Permission denied | Report to user, do not retry |
| Git not available | Skip git-dependent steps, warn "reduced accuracy" |
| Network timeout | Retry once after 5s, then skip with warning |
| Parse/syntax error in project files | Report file and line, continue with other files |

3. Never silently swallow errors - always surface in conversation.
4. If 2+ scripts fail in same workflow, pause and ask user before continuing.

## Systematic Debugging Protocol

**Iron Law: NO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST.**

When encountering bugs, test failures, or unexpected behavior - follow these 4 phases IN ORDER:

### Phase 1: Root Cause Investigation (BEFORE any fix)

1. Read error messages COMPLETELY (stack trace, line numbers, error codes)
2. Reproduce consistently - can you trigger it reliably?
3. Check recent changes - `git diff`, recent commits, new dependencies
4. Trace data flow - where does the bad value originate?

### Phase 2: Pattern Analysis

1. Find working examples of similar code in the codebase
2. Compare working vs broken - list EVERY difference
3. Don't assume "that can't matter"

### Phase 3: Hypothesis & Testing

1. Form ONE hypothesis: "I think X causes Y because Z"
2. Make the SMALLEST possible change to test it
3. Did it work? -> Phase 4. Didn't work? -> New hypothesis
4. Do NOT stack multiple fixes

### Phase 4: Implementation

1. Write a failing test reproducing the bug
2. Implement single fix addressing root cause
3. Verify fix: test passes, no regressions

### 3-Fix Architecture Circuit Breaker

If 3+ fix attempts have failed:
- **STOP.** This is likely an architectural problem, not a code bug
- Question fundamentals: Is this pattern sound? Are we fighting inertia?
- Discuss with user before attempting more fixes

### Red Flags - STOP and Return to Phase 1

- "Quick fix for now, investigate later"
- "Just try changing X and see"
- "I don't fully understand but this might work"
- Proposing solutions before tracing data flow

## Circuit Breaker Protocol

When `run_gate.py` returns `consecutive_failures >= 3`:
1. **HALT execution.** Do not attempt another automatic fix.
2. Read `.codex/decisions/` to check if a recent architectural change broke the tests.
3. Switch behavioral mode to `devil's-advocate` and critically evaluate if the foundational approach is wrong.
4. Present the user with two options:
   - "Continue debugging with a new approach"
   - "Revert the last commits and return to Planning phase"
5. Reset the counter only after gate passes or user explicitly requests `$reset-gate`.

## Complexity-to-Scope Mapping

| Intent Analyzer Output | Workflow Autopilot Scope | Plan Writer Trigger | Action |
| --- | --- | --- | --- |
| `complexity: simple` | `estimated_scope: small` | Skip plan (direct execution) | Standard |
| `complexity: complex` + <=10 files | `estimated_scope: medium` | Plan recommended | Standard |
| `complexity: complex` + >10 files | `estimated_scope: large` | Plan mandatory | Staged execution |
| Blast Radius > 20 files | `estimated_scope: epic` | **HALT** | Epic Mode |

## Epic Mode

When `predict_impact.py` returns `escalate_to_epic: true`:
1. **Refuse** to write implementation code in the current session.
2. Generate a "Master Plan" document breaking the epic into 3-5 isolated tickets.
3. Each ticket must have:
   - Clear file boundary (which files belong to this ticket)
   - Independent acceptance criteria
   - Estimated blast radius <= 15 files
4. Ask the user to approve the Master Plan.
5. Instruct the user to open a **fresh session** for each ticket to preserve context window.

## Script Invocation Discipline

1. Always run `--help` before invoking any helper script.
2. Treat scripts as black-box tools; execute by CLI contract first.
3. Read script source only when customization or bug fixing is required.

## Cross-Reference Table

| Task Type | Preferred Workflow | Suggested Scripts |
| --- | --- | --- |
| new feature | `codex-workflow-autopilot/references/workflow-create.md` | `predict_impact.py`, `pre_commit_check.py`, `smart_test_selector.py`, `suggest_improvements.py` |
| bug fix | `codex-workflow-autopilot/references/workflow-debug.md` | `pre_commit_check.py`, `smart_test_selector.py`, `track_feedback.py` |
| code review/audit | `codex-workflow-autopilot/references/workflow-review.md` | `tech_debt_scan.py`, `quality_trend.py --report`, `security_scan.py` |
| refactor | `codex-workflow-autopilot/references/workflow-refactor.md` | `tech_debt_scan.py`, `predict_impact.py`, `pre_commit_check.py`, `smart_test_selector.py`, `suggest_improvements.py` |
| deploy/ship | `codex-workflow-autopilot/references/workflow-deploy.md` | `security_scan.py`, `bundle_check.py`, `lighthouse_audit.py`, `playwright_runner.py`, `generate_changelog.py`, `with_server.py` |
| environment check | pre-flight diagnostics | `doctor.py` |
| session handoff | `codex-workflow-autopilot/references/workflow-handoff.md` | `generate_session_summary.py`, `generate_handoff.py`, `decision_logger.py`, `generate_changelog.py`, `track_feedback.py` |
| docs sync | workflow docs phase | `map_changes_to_docs.py`, `generate_changelog.py` |
| release/pre-ship | quality gate + ship mode | `security_scan.py`, `lighthouse_audit.py`, `playwright_runner.py`, `with_server.py` |
| long-running project continuity | project-memory flow | `decision_logger.py`, `generate_session_summary.py`, `generate_handoff.py`, `generate_growth_report.py` |

## Two-Stage Review Protocol

When reviewing completed work against a plan or spec:

### Stage 1: Spec Compliance Review (DO FIRST)

- Does the implementation match EVERY requirement in the plan/spec?
- Is anything MISSING that was requested?
- Is anything EXTRA that was NOT requested (YAGNI violation)?
- Are acceptance criteria met?

⚠️ Do NOT proceed to Stage 2 until Stage 1 passes.

### Stage 2: Code Quality Review (DO SECOND)

- Does the code follow project conventions?
- Is error handling proper?
- Are there security concerns?
- Is test coverage adequate?
- Is there unnecessary complexity?

### Issue Severity

- **Critical** (must fix before completion) - missing requirements, security holes, data loss risk
- **Important** (should fix) - poor error handling, missing tests, convention violations
- **Suggestion** (nice to have) - naming improvements, minor refactoring opportunities

## Quality Gate Decision Tree

```
Task type -> Code change?
    |- Yes -> What kind?
    |   |- New feature -> run: pre_commit_check + smart_test_selector + predict_impact
    |   |- Bug fix -> run: pre_commit_check + smart_test_selector
    |   |- Refactor -> run: tech_debt_scan + pre_commit_check
    |   `- UI change -> run: ux_audit + accessibility_check + pre_commit_check
    |
    |- Deploy/ship? -> run: security_scan + lighthouse_audit + playwright_runner
    |
    |- Review/audit? -> run: quality_trend + suggest_improvements + tech_debt_scan
    |
    `- No code -> skip quality gate
```

## Core Script Inventory (30)

| Script | Purpose | Usage |
| --- | --- | --- |
| `map_changes_to_docs.py` | map code changes to docs candidates | `python "...\\codex-docs-change-sync\\scripts\\map_changes_to_docs.py" --project-root <path> --diff-scope auto` |
| `auto_commit.py` | task-scoped auto-commit with gate, GPG sign, and push | `python "...\\codex-git-autopilot\\scripts\\auto_commit.py" --project-root <path> --files <file1> <file2>` |
| `run_gate.py` | lint/test gate evaluation | `python "...\\codex-execution-quality-gate\\scripts\\run_gate.py" --project-root <path>` |
| `doctor.py` | Environment tool dependency check | `python "...\\codex-execution-quality-gate\\scripts\\doctor.py"` |
| `security_scan.py` | static security checks | `python "...\\codex-execution-quality-gate\\scripts\\security_scan.py" --project-root <path>` |
| `bundle_check.py` | dependency/bundle risk checks | `python "...\\codex-execution-quality-gate\\scripts\\bundle_check.py" --project-root <path>` |
| `tech_debt_scan.py` | tech debt signal scan | `python "...\\codex-execution-quality-gate\\scripts\\tech_debt_scan.py" --project-root <path>` |
| `pre_commit_check.py` | staged-file pre-commit checks | `python "...\\codex-execution-quality-gate\\scripts\\pre_commit_check.py" --project-root <path>` |
| `smart_test_selector.py` | select related tests from changes | `python "...\\codex-execution-quality-gate\\scripts\\smart_test_selector.py" --project-root <path> --source staged` |
| `suggest_improvements.py` | post-task improvement suggestions | `python "...\\codex-execution-quality-gate\\scripts\\suggest_improvements.py" --project-root <path>` |
| `predict_impact.py` | pre-edit blast-radius prediction | `python "...\\codex-execution-quality-gate\\scripts\\predict_impact.py" --project-root <path> --files <file1,file2>` |
| `quality_trend.py` | quality snapshots and trend reports | `python "...\\codex-execution-quality-gate\\scripts\\quality_trend.py" --project-root <path> --report` |
| `ux_audit.py` | static UX anti-pattern audit | `python "...\\codex-execution-quality-gate\\scripts\\ux_audit.py" --project-root <path>` |
| `accessibility_check.py` | static WCAG checks | `python "...\\codex-execution-quality-gate\\scripts\\accessibility_check.py" --project-root <path> --level AA` |
| `lighthouse_audit.py` | Lighthouse wrapper for runtime audits | `python "...\\codex-execution-quality-gate\\scripts\\lighthouse_audit.py" --url http://localhost:3000` |
| `playwright_runner.py` | Playwright check/generate/run wrapper | `python "...\\codex-execution-quality-gate\\scripts\\playwright_runner.py" --project-root <path> --mode check` |
| `with_server.py` | start server(s), wait ports, run command, cleanup | `python "...\\codex-execution-quality-gate\\scripts\\with_server.py" --server \"npm run dev\" --port 3000 -- python <cmd>` |
| `decision_logger.py` | log architecture decisions | `python "...\\codex-project-memory\\scripts\\decision_logger.py" --project-root <path> --title <slug> --decision <text> --alternatives <text> --reasoning <text> --context <text>` |
| `generate_handoff.py` | export portable project handoff | `python "...\\codex-project-memory\\scripts\\generate_handoff.py" --project-root <path>` |
| `generate_session_summary.py` | summarize session changes | `python "...\\codex-project-memory\\scripts\\generate_session_summary.py" --project-root <path> --since today` |
| `analyze_patterns.py` | learn project coding conventions | `python "...\\codex-project-memory\\scripts\\analyze_patterns.py" --project-root <path>` |
| `track_feedback.py` | log/aggregate AI feedback corrections | `python "...\\codex-project-memory\\scripts\\track_feedback.py" --project-root <path> --aggregate` |
| `track_skill_usage.py` | usage analytics for skill effectiveness | `python "...\\codex-project-memory\\scripts\\track_skill_usage.py" --skills-root <skills-root> --report` |
| `build_knowledge_graph.py` | build dependency/data-flow graph | `python "...\\codex-project-memory\\scripts\\build_knowledge_graph.py" --project-root <path>` |
| `generate_genome.py` | Generate multi-layer project context docs | `python "...\\codex-project-memory\\scripts\\generate_genome.py" --project-root <path>` |
| `generate_changelog.py` | generate user-facing changelog from commits | `python "...\\codex-project-memory\\scripts\\generate_changelog.py" --project-root <path> --since \"30 days ago\"` |
| `generate_growth_report.py` | aggregate feedback/session/usage growth report | `python "...\\codex-project-memory\\scripts\\generate_growth_report.py" --project-root <path> --skills-root <skills-root>` |
| `compact_context.py` | Archive old memory files to reduce context | `python "...\\codex-project-memory\\scripts\\compact_context.py" --project-root <path>` |
| `explain_code.py` | teaching-mode code context extractor | `python "...\\codex-workflow-autopilot\\scripts\\explain_code.py" --project-root <path> --file <file>` |
| `render_docx.py` | DOCX to image rendering | `python "...\\codex-doc-renderer\\scripts\\render_docx.py" <path>` |

## Workflow References

- `codex-workflow-autopilot/references/workflow-create.md`
- `codex-workflow-autopilot/references/workflow-debug.md`
- `codex-workflow-autopilot/references/workflow-review.md`
- `codex-workflow-autopilot/references/workflow-refactor.md`
- `codex-workflow-autopilot/references/workflow-deploy.md`
- `codex-workflow-autopilot/references/workflow-handoff.md`
